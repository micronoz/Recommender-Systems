{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "from math import log\n",
    "import re\n",
    "from random import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "### Just the first 5000 reviews\n",
    "\n",
    "print (\"Reading data...\")\n",
    "fullData = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "data = fullData[:5000]\n",
    "shuffle(fullData)\n",
    "trainData = fullData[:5000]\n",
    "valData = fullData[5000:10000]\n",
    "testData = fullData[10000:]\n",
    "print (\"done\")\n",
    "translator = str.maketrans(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36225\n"
     ]
    }
   ],
   "source": [
    "### How many unique words are there?\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "for d in data:\n",
    "    for w in d['review/text'].split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount))\n",
    "\n",
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "        wordCount[w] += 1\n",
    "\n",
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "toBeFound = words.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of unique bigrams: 182246\n",
      "Top 5 bigrams:\n",
      "Bigram ('with', 'a') with count 4587\n",
      "Bigram ('in', 'the') with count 2595\n",
      "Bigram ('of', 'the') with count 2245\n",
      "Bigram ('is', 'a') with count 2056\n",
      "Bigram ('on', 'the') with count 2033\n",
      "\n",
      "MSE of unigram model: 0.2787546353022137\n",
      "\n",
      "MSE of bigram model: 0.34288593369587056\n"
     ]
    }
   ],
   "source": [
    "### Bigram counts\n",
    "bigramCount = defaultdict(int)\n",
    "reviewBigramContent = []\n",
    "for d in data:\n",
    "    text = d['review/text']\n",
    "    removed = text.translate(translator)\n",
    "    lowered = removed.lower()\n",
    "    wordList = lowered.split()\n",
    "    prev = None\n",
    "    reviewSet = []\n",
    "    for w in wordList:\n",
    "        if (prev is None):\n",
    "            pass\n",
    "        else:\n",
    "            bigramCount[(prev,w)] += 1\n",
    "            reviewSet.append((prev,w))\n",
    "        prev = w\n",
    "    reviewBigramContent.append(reviewSet)\n",
    "frequentWords = sorted(list(bigramCount.items()), key=operator.itemgetter(1))\n",
    "frequentWords.reverse()\n",
    "bigrams = set(bigramCount.keys())\n",
    "bigramNoDup = list(bigrams)\n",
    "print('')\n",
    "print('Total number of unique bigrams: {}'.format(len(frequentWords)))\n",
    "print('Top 5 bigrams:')\n",
    "for i in range(5):\n",
    "    print('Bigram {} with count {}'.format(frequentWords[i][0],frequentWords[i][1]))\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum, wordKey):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordKey[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d,wordId) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "err = mean_squared_error(y,predictions)\n",
    "print('\\nMSE of unigram model: {}'.format(err))\n",
    "bigramsWithCounts = frequentWords[:1000]\n",
    "bigrams = []\n",
    "for v in bigramsWithCounts:\n",
    "    bigrams.append(v[0])\n",
    "### Bigram model\n",
    "def featureBi(bigramList,wordKey):\n",
    "    feat = [0] * len(bigrams)\n",
    "    for w in bigramList:\n",
    "        if w in bigrams:\n",
    "            feat[wordKey[w]] += 1\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "wordId = dict(zip(bigrams, range(len(bigrams))))\n",
    "X = [featureBi(d,wordId) for d in reviewBigramContent]\n",
    "y = [d['review/overall'] for d in data]\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X,y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "err = mean_squared_error(y,predictions)\n",
    "print('\\nMSE of bigram model: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsAndWords = []\n",
    "allWords = set()\n",
    "for d in data:\n",
    "    text = d['review/text']\n",
    "    words = ((text.translate(translator)).lower()).split()\n",
    "    reviewsAndWords.append(words)\n",
    "    for w in words:\n",
    "        allWords.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkWords = ['foam','smell','banana','lactic','tart']\n",
    "totalDocuments = len(reviewsAndWords)\n",
    "inverseFrequencies = defaultdict(float)\n",
    "for word in allWords:\n",
    "    doc = 0\n",
    "    for rev in reviewsAndWords:\n",
    "        if word in rev:\n",
    "            doc += 1\n",
    "    inverseFrequencies[word] = log(totalDocuments/doc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF score for word \"foam\": 1.1378686206869628\n",
      "IDF score for word \"smell\": 0.5379016188648442\n",
      "IDF score for word \"banana\": 1.6777807052660807\n",
      "IDF score for word \"lactic\": 2.920818753952375\n",
      "IDF score for word \"tart\": 1.8068754016455382\n"
     ]
    }
   ],
   "source": [
    "for word in checkWords:\n",
    "    print('IDF score for word \"{}\": {}'.format(word,inverseFrequencies[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF score for word \"foam\" in first document: 2.2757372413739256\n",
      "TF-IDF score for word \"smell\" in first document: 0.5379016188648442\n",
      "TF-IDF score for word \"banana\" in first document: 3.3555614105321614\n",
      "TF-IDF score for word \"lactic\" in first document: 5.84163750790475\n",
      "TF-IDF score for word \"tart\" in first document: 1.8068754016455382\n"
     ]
    }
   ],
   "source": [
    "for word in checkWords:\n",
    "    c = reviewsAndWords[0].count(word)\n",
    "    print('TF-IDF score for word \"{}\" in first document: {}'.format(word,c*inverseFrequencies[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity of first and second review: 0.06588193974744382\n"
     ]
    }
   ],
   "source": [
    "v1 = []\n",
    "v2 = []\n",
    "for word in allWords:\n",
    "    c1 = reviewsAndWords[0].count(word)\n",
    "    c2 = reviewsAndWords[1].count(word)\n",
    "    v1.append(c1*inverseFrequencies[word])\n",
    "    v2.append(c2*inverseFrequencies[word])\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "print('Cosine similarity of first and second review: {}'.format(cos_sim(v1,v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i in range(len(reviewsAndWords)):\n",
    "    v = []\n",
    "    for word in allWords:\n",
    "        c = reviewsAndWords[i].count(word)\n",
    "        v.append(c*inverseFrequencies[word])\n",
    "    vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review with highest cosine similarity to first review has beerId \"72146\" and profileName \"spicelab\"\n"
     ]
    }
   ],
   "source": [
    "v1 = vectors[0]\n",
    "\n",
    "maxCos = 0\n",
    "index = 1\n",
    "maxIndex = -1\n",
    "for v in vectors[1:]:\n",
    "    cos = cos_sim(v1,v)\n",
    "    if cos > maxCos:\n",
    "        maxCos = cos\n",
    "        maxIndex = index\n",
    "    index += 1\n",
    "print('Review with highest cosine similarity to first review has beerId \"{}\" and profileName \"{}\"'.format(data[maxIndex]['beer/beerId'], data[maxIndex]['user/profileName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of TF-IDF model: 0.27875956007772285\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i in range(len(reviewsAndWords)):\n",
    "    v = []\n",
    "    v.append(1)\n",
    "    for word in toBeFound:\n",
    "        c = reviewsAndWords[i].count(word)\n",
    "        v.append(c*inverseFrequencies[word])\n",
    "    vectors.append(v)\n",
    "X = vectors.copy()\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "clf = linear_model.Ridge(1, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "err = mean_squared_error(y,predictions)\n",
    "print('MSE of TF-IDF model: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model testing\n",
    "def getData(reviews, train, uni, punc, tfidf, trainWords=None):\n",
    "    revWords = []\n",
    "    allWords = set()\n",
    "    for review in reviews:\n",
    "        text = review['review/text']\n",
    "        if not punc:\n",
    "            words = ((text.translate(translator)).lower()).split()\n",
    "        else:\n",
    "            words = re.findall(r\"[\\w']+|[.,!?;]\",text.lower())\n",
    "        revWords.append(words)\n",
    "    if uni:\n",
    "        revCounts = []\n",
    "        wordInverseFreq = defaultdict(int)\n",
    "        for rev in revWords:\n",
    "            wordCounts = defaultdict(int)\n",
    "            for word in rev:\n",
    "                allWords.add(word)\n",
    "                wordCounts[word] += 1\n",
    "            revCounts.append(wordCounts)\n",
    "        total = len(reviews)\n",
    "        for word in allWords:\n",
    "            count = 0\n",
    "            for rev in revCounts:\n",
    "                if word in rev:\n",
    "                    count += 1\n",
    "            wordInverseFreq[word] = log(total/count,10)\n",
    "    else:\n",
    "        revCounts = []\n",
    "        wordInverseFreq = defaultdict(int)\n",
    "        for rev in revWords:\n",
    "            pairCounts = defaultdict(int)\n",
    "            for i in range(len(rev)-1):\n",
    "                pair = (rev[i], rev[i+1])\n",
    "                allWords.add(pair)\n",
    "                pairCounts[pair] += 1\n",
    "            revCounts.append(pairCounts)\n",
    "        total = len(reviews)\n",
    "        for pair in allWords:\n",
    "            count = 0\n",
    "            for rev in revCounts:\n",
    "                if pair in rev:\n",
    "                    count += 1\n",
    "            wordInverseFreq[pair] = log(total/count,10)\n",
    "    if not tfidf:\n",
    "        if train:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                feat.append(1)\n",
    "                for word in allWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "        else:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                feat.append(1)\n",
    "                for word in trainWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "    else:\n",
    "        if train:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                feat.append(1)\n",
    "                for word in allWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word] * wordInverseFreq[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "        else:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                feat.append(1)\n",
    "                for word in trainWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word] * wordInverseFreq[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "    return X, list(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(unigram,punctuation,tfidf):\n",
    "    X_train, words = getData(trainData, True, unigram,punctuation,tfidf)\n",
    "    y_train = [r['review/overall'] for r in trainData]\n",
    "    X_val, _ = getData(valData,False,unigram,punctuation,tfidf,words)\n",
    "    y_val = [r['review/overall'] for r in valData]\n",
    "    regularizers = [0.01,0.1,1,10,100]\n",
    "    for reg in regularizers:\n",
    "        clf = linear_model.Ridge(reg, fit_intercept=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        theta = clf.coef_\n",
    "        predictions = clf.predict(X_val)\n",
    "        err = mean_squared_error(y_val,predictions)\n",
    "        print('Words: {}, Punctuation: {}, Model: {}, Regularizer: {}, MSE: {}'.format('Unigram' if unigram else 'Bigram','Kept' if punctuation else 'Removed','TF-IDF' if tfidf else 'Counts',reg,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: Bigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 0.01, MSE: 1.8862181310759654\n",
      "Words: Bigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 0.1, MSE: 1.8861658649250537\n",
      "Words: Bigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 1, MSE: 1.885647097566243\n",
      "Words: Bigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 10, MSE: 1.8808223634720744\n",
      "Words: Bigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 100, MSE: 1.8548149170135138\n",
      "Words: Bigram, Punctuation: Kept, Model: Counts, Regularizer: 0.01, MSE: 2.0245113627375444\n",
      "Words: Bigram, Punctuation: Kept, Model: Counts, Regularizer: 0.1, MSE: 2.023304775836114\n",
      "Words: Bigram, Punctuation: Kept, Model: Counts, Regularizer: 1, MSE: 2.0118209329540218\n",
      "Words: Bigram, Punctuation: Kept, Model: Counts, Regularizer: 10, MSE: 1.933035297458138\n",
      "Words: Bigram, Punctuation: Kept, Model: Counts, Regularizer: 100, MSE: 1.794753905512156\n",
      "Words: Bigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 0.01, MSE: 2.167417092541125\n",
      "Words: Bigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 0.1, MSE: 2.167395081397154\n",
      "Words: Bigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 1, MSE: 2.167177458626135\n",
      "Words: Bigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 10, MSE: 2.165233997268316\n",
      "Words: Bigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 100, MSE: 2.1604211159781204\n",
      "Words: Bigram, Punctuation: Removed, Model: Counts, Regularizer: 0.01, MSE: 2.3031320540739304\n",
      "Words: Bigram, Punctuation: Removed, Model: Counts, Regularizer: 0.1, MSE: 2.302177299615284\n",
      "Words: Bigram, Punctuation: Removed, Model: Counts, Regularizer: 1, MSE: 2.2931072880386636\n",
      "Words: Bigram, Punctuation: Removed, Model: Counts, Regularizer: 10, MSE: 2.2325865651085564\n",
      "Words: Bigram, Punctuation: Removed, Model: Counts, Regularizer: 100, MSE: 2.1809703271148106\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 0.01, MSE: 4.01055826797125\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 0.1, MSE: 3.948424080348474\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 1, MSE: 3.5364562233438903\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 10, MSE: 2.568415049160528\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 100, MSE: 1.8861795490128672\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 0.01, MSE: 4.322541025784355\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 0.1, MSE: 3.891389329026427\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 1, MSE: 2.7900857443768685\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 10, MSE: 1.9151691017847603\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 100, MSE: 1.607167223096318\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 0.01, MSE: 3.4711037409717\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 0.1, MSE: 3.4379850742336084\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 1, MSE: 3.196840379015003\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 10, MSE: 2.495611933195827\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 100, MSE: 1.9102693980827168\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 0.01, MSE: 3.786931886877045\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 0.1, MSE: 3.534393088459552\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 1, MSE: 2.7259494347253814\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 10, MSE: 1.94142584543845\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 100, MSE: 1.6493934955017735\n"
     ]
    }
   ],
   "source": [
    "unigram = [False,True]\n",
    "punctuation = [True,False]\n",
    "tfidf = [True,False]\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "processes = []\n",
    "\n",
    "for j in punctuation:\n",
    "    for k in tfidf:\n",
    "        train_and_test(False,j,k)\n",
    "\n",
    "for j in punctuation:\n",
    "    for k in tfidf:\n",
    "        p = Process(target=train_and_test, args=((True,j,k)))\n",
    "        p.start()\n",
    "        p.join()\n",
    "        processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 100, MSE: 1.6688250630614938\n"
     ]
    }
   ],
   "source": [
    "### Best model's performance on the test set\n",
    "X_train, words = getData(trainData, True, True,True,False)\n",
    "y_train = [r['review/overall'] for r in trainData]\n",
    "X_val, _ = getData(testData,False,True,True,False,words)\n",
    "y_val = [r['review/overall'] for r in testData]\n",
    "clf = linear_model.Ridge(100, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_val)\n",
    "err = mean_squared_error(y_val,predictions)\n",
    "print('Words: {}, Punctuation: {}, Model: {}, Regularizer: {}, MSE: {}'.format('Unigram','Kept','Counts',100,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
