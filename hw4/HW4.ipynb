{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "from math import log\n",
    "import re\n",
    "from random import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "### Just the first 5000 reviews\n",
    "\n",
    "print (\"Reading data...\")\n",
    "fullData = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "data = fullData[:5000]\n",
    "shuffle(fullData)\n",
    "trainData = fullData[:5000]\n",
    "valData = fullData[5000:10000]\n",
    "testData = fullData[10000:]\n",
    "print (\"done\")\n",
    "translator = str.maketrans(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many unique words are there?\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "for d in data:\n",
    "    for w in d['review/text'].split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount))\n",
    "\n",
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "        wordCount[w] += 1\n",
    "\n",
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "toBeFound = words.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bigram counts\n",
    "bigramCount = defaultdict(int)\n",
    "reviewBigramContent = []\n",
    "for d in data:\n",
    "    text = d['review/text']\n",
    "    removed = text.translate(translator)\n",
    "    lowered = removed.lower()\n",
    "    wordList = lowered.split()\n",
    "    prev = None\n",
    "    reviewSet = []\n",
    "    for w in wordList:\n",
    "        if (prev is None):\n",
    "            bigramCount[('/',w)] += 1\n",
    "            reviewSet.append(('/',w))\n",
    "        else:\n",
    "            bigramCount[(prev,w)] += 1\n",
    "            reviewSet.append((prev,w))\n",
    "        prev = w\n",
    "    reviewBigramContent.append(reviewSet)\n",
    "frequentWords = sorted(list(bigramCount.items()), key=operator.itemgetter(1))\n",
    "frequentWords.reverse()\n",
    "bigrams = set(bigramCount.keys())\n",
    "bigramNoDup = list(bigrams)\n",
    "print('')\n",
    "print('Total number of unique bigrams: {}'.format(len(frequentWords)))\n",
    "print('Top 5 bigrams:')\n",
    "for i in range(5):\n",
    "    print('Bigram {} with count {}'.format(frequentWords[i][0],frequentWords[i][1]))\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum, wordKey):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordKey[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d,wordId) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "err = mean_squared_error(y,predictions)\n",
    "print('\\nMSE of unigram model: {}'.format(err))\n",
    "\n",
    "### Bigram model\n",
    "def featureBi(bigramList,wordKey):\n",
    "    feat = [0] * len(bigrams)\n",
    "    for w in bigramList:\n",
    "        feat[wordKey[w]] += 1\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "wordId = dict(zip(bigramNoDup, range(len(bigramNoDup))))\n",
    "X = [featureBi(d,wordId) for d in reviewBigramContent]\n",
    "y = [d['review/overall'] for d in data]\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X,y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "err = mean_squared_error(y,predictions)\n",
    "print('\\nMSE of bigram model: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsAndWords = []\n",
    "allWords = set()\n",
    "for d in data:\n",
    "    text = d['review/text']\n",
    "    words = ((text.translate(translator)).lower()).split()\n",
    "    reviewsAndWords.append(words)\n",
    "    for w in words:\n",
    "        allWords.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkWords = ['foam','smell','banana','lactic','tart']\n",
    "totalDocuments = len(reviewsAndWords)\n",
    "inverseFrequencies = defaultdict(float)\n",
    "for word in allWords:\n",
    "    doc = 0\n",
    "    for rev in reviewsAndWords:\n",
    "        if word in rev:\n",
    "            doc += 1\n",
    "    inverseFrequencies[word] = log(totalDocuments/doc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in checkWords:\n",
    "    print('IDF score for word \"{}\": {}'.format(word,inverseFrequencies[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in checkWords:\n",
    "    c = reviewsAndWords[0].count(word)\n",
    "    print('TF-IDF score for word \"{}\" in first document: {}'.format(word,c*inverseFrequencies[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = []\n",
    "v2 = []\n",
    "for word in allWords:\n",
    "    c1 = reviewsAndWords[0].count(word)\n",
    "    c2 = reviewsAndWords[1].count(word)\n",
    "    v1.append(c1*inverseFrequencies[word])\n",
    "    v2.append(c2*inverseFrequencies[word])\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "print('Cosine similarity of first and second review: {}'.format(cos_sim(v1,v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i in range(len(reviewsAndWords)):\n",
    "    v = []\n",
    "    for word in allWords:\n",
    "        c = reviewsAndWords[i].count(word)\n",
    "        v.append(c*inverseFrequencies[word])\n",
    "    vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = vectors[0]\n",
    "\n",
    "maxCos = 0\n",
    "index = 1\n",
    "maxIndex = -1\n",
    "for v in vectors[1:]:\n",
    "    cos = cos_sim(v1,v)\n",
    "    if cos > maxCos:\n",
    "        maxCos = cos\n",
    "        maxIndex = index\n",
    "    index += 1\n",
    "print('Review with highest cosine similarity to first review has beerId \"{}\" and profileName \"{}\"'.format(data[maxIndex]['beer/beerId'], data[maxIndex]['user/profileName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i in range(len(reviewsAndWords)):\n",
    "    v = []\n",
    "    v.append(1)\n",
    "    for word in toBeFound:\n",
    "        c = reviewsAndWords[i].count(word)\n",
    "        v.append(c*inverseFrequencies[word])\n",
    "    vectors.append(v)\n",
    "X = vectors.copy()\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "clf = linear_model.Ridge(1, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "err = mean_squared_error(y,predictions)\n",
    "print('MSE of TF-IDF model: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model testing\n",
    "def getData(reviews, train, uni, punc, tfidf, trainWords=None):\n",
    "    revWords = []\n",
    "    allWords = set()\n",
    "    for review in reviews:\n",
    "        text = review['review/text']\n",
    "        if not punc:\n",
    "            words = ((text.translate(translator)).lower()).split()\n",
    "        else:\n",
    "            words = re.findall(r\"[\\w']+|[.,!?;]\",text.lower())\n",
    "        revWords.append(words)\n",
    "    if uni:\n",
    "        revCounts = []\n",
    "        wordInverseFreq = defaultdict(int)\n",
    "        for rev in revWords:\n",
    "            wordCounts = defaultdict(int)\n",
    "            for word in rev:\n",
    "                allWords.add(word)\n",
    "                wordCounts[word] += 1\n",
    "            revCounts.append(wordCounts)\n",
    "        total = len(reviews)\n",
    "        for word in allWords:\n",
    "            count = 0\n",
    "            for rev in revCounts:\n",
    "                if word in rev:\n",
    "                    count += 1\n",
    "            wordInverseFreq[word] = log(total/count,10)\n",
    "    else:\n",
    "        revCounts = []\n",
    "        wordInverseFreq = defaultdict(int)\n",
    "        for rev in revWords:\n",
    "            pairCounts = defaultdict(int)\n",
    "            for i in range(len(rev)-1):\n",
    "                pair = (rev[i], rev[i+1])\n",
    "                allWords.add(pair)\n",
    "                pairCounts[pair] += 1\n",
    "            revCounts.append(pairCounts)\n",
    "        total = len(reviews)\n",
    "        for pair in allWords:\n",
    "            count = 0\n",
    "            for rev in revCounts:\n",
    "                if pair in rev:\n",
    "                    count += 1\n",
    "            wordInverseFreq[pair] = log(total/count,10)\n",
    "    if not tfidf:\n",
    "        if train:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                for word in allWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "        else:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                for word in trainWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "    else:\n",
    "        if train:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                for word in allWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word] * wordInverseFreq[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "        else:\n",
    "            X = []\n",
    "            for rev in revCounts:\n",
    "                feat = []\n",
    "                for word in trainWords:\n",
    "                    if word in rev.keys():\n",
    "                        feat.append(rev[word] * wordInverseFreq[word])\n",
    "                    else:\n",
    "                        feat.append(0)\n",
    "                X.append(feat)\n",
    "    return X, list(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(unigram,punctuation,tfidf):\n",
    "    X_train, words = getData(trainData, True, unigram,punctuation,tfidf)\n",
    "    y_train = [r['review/overall'] for r in trainData]\n",
    "    X_val, _ = getData(valData,False,unigram,punctuation,tfidf,words)\n",
    "    y_val = [r['review/overall'] for r in valData]\n",
    "    regularizers = [0.01,0.1,1,10,100]\n",
    "    for reg in regularizers:\n",
    "        clf = linear_model.Ridge(reg, fit_intercept=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        theta = clf.coef_\n",
    "        predictions = clf.predict(X_val)\n",
    "        err = mean_squared_error(y_val,predictions)\n",
    "        print('Words: {}, Punctuation: {}, Model: {}, Regularizer: {}, MSE: {}'.format('Unigram' if unigram else 'Bigram','Kept' if punctuation else 'Removed','TF-IDF' if tfidf else 'Counts',reg,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 0.01, MSE: 5.63272659055397\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 0.01, MSE: 5.6225554273653815\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 0.1, MSE: 4.440069093926033\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 0.01, MSE: 5.1354502679498\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 0.01, MSE: 5.12521897103564\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 0.1, MSE: 5.169067309977103\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 1, MSE: 3.1629801042908183\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 1, MSE: 4.067827026264244\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 0.1, MSE: 4.193406353073641\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 0.1, MSE: 4.787854162558175\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 10, MSE: 2.111156271908436\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 10, MSE: 2.896319329466294\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 1, MSE: 3.2357575771938163\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 1, MSE: 3.921714479034725\n",
      "Words: Unigram, Punctuation: Kept, Model: Counts, Regularizer: 100, MSE: 1.683239310523554\n",
      "Words: Unigram, Punctuation: Kept, Model: TF-IDF, Regularizer: 100, MSE: 2.0606006924537352\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 10, MSE: 2.2779488252146365\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 10, MSE: 3.007617943753243\n",
      "Words: Unigram, Punctuation: Removed, Model: Counts, Regularizer: 100, MSE: 1.8149001697306728\n",
      "Words: Unigram, Punctuation: Removed, Model: TF-IDF, Regularizer: 100, MSE: 2.22114561959355\n"
     ]
    }
   ],
   "source": [
    "unigram = [True,False]\n",
    "punctuation = [True,False]\n",
    "tfidf = [True,False]\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "processes = []\n",
    "for i in unigram:\n",
    "    for j in punctuation:\n",
    "        for k in tfidf:\n",
    "            p = Process(target=train_and_test, args=((i,j,k)))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
